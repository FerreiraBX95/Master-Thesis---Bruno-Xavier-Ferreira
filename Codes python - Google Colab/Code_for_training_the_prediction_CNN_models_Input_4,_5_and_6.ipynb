{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14f-OJIeNlGE3dikwfYymo-8YHYlEiey4",
      "authorship_tag": "ABX9TyO2IvvKKk+7UWO0iPl1ut2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FerreiraBX95/Master-Thesis---Bruno-Xavier-Ferreira/blob/Development-of-models-for-measurement-of-pH-in-atmospheric-and-pressurized-systems-using-artificial-intelligent-strategies/Code_for_training_the_prediction_CNN_models_Input_4%2C_5_and_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECaZA7FPMvOt"
      },
      "source": [
        "*** Code for training the prediction CNN models - Input 4, 5 and 6***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPzy34xmM4_O"
      },
      "source": [
        "Inicialização do Weights and Bias (WandB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pAqoNHKMuNW"
      },
      "source": [
        "# WandB – Install the W&B library\n",
        "%pip install wandb -q\n",
        "#!pip install wandb --upgrade\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVdrkQb4eXcQ"
      },
      "source": [
        "Informações da GPU a ser utilizada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp94ktVkd0P8"
      },
      "source": [
        "# Para usar a GPU eh preciso antes \"Alterar o tipo de ambiene de Execução\" para GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print('Informacao da GPU: ')\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5TMgnx6NJdT"
      },
      "source": [
        "Atualizações de programas (se necessario)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbeD6vlbMSg-"
      },
      "source": [
        "%pip install scikit-plot -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RsK5G-TvnHi"
      },
      "source": [
        "%pip install scikit-learn -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIGnXWulNKek"
      },
      "source": [
        "#!pip install keras --upgrade\n",
        "#!pip install tensorflow --upgrade\n",
        "#!pip install sklearn --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB7F4TUL19Vk"
      },
      "source": [
        "%pip install keras-metrics -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ftDi03NNTE"
      },
      "source": [
        "Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akyaH1BCNRlo"
      },
      "source": [
        "# Bibliotecas\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score \n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.metrics import fbeta_score \n",
        "from sklearn.metrics import precision_score \n",
        "from sklearn.metrics import recall_score \n",
        "\n",
        "from skimage.transform import resize\n",
        "from scikitplot.metrics import plot_roc\n",
        "from scikitplot.metrics import plot_confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from skimage import data\n",
        "from skimage.color import rgb2hsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLo0keISNZs1"
      },
      "source": [
        "Carregamento e pre-processamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "568rF2FENasx"
      },
      "source": [
        "## Teste para leitura de imagens do drive\n",
        "address = '/content/drive/MyDrive/Mestrado_Bruno Xavier/Dissertacao de Mestrado - Cod/Banco_de_Imagens_Curva_de_calib_PYTHON_med_pH_Alline_ajust'\n",
        "CATEGORIES = [\"pH_2\",\"pH_3\",\"pH_4\",\"pH_5\",\"pH_6\",\"pH_7\",\"pH_8\",\"pH_9\",\"pH_10\"]\n",
        "\n",
        "tam = 313\n",
        "\n",
        "#x_db = np.zeros((tam,800,1280),np.int16)\n",
        "x_db = np.zeros((tam,280,280),np.int16)\n",
        "\n",
        "ind = 0\n",
        "for category in CATEGORIES:  \n",
        "    path = os.path.join(address,category)  # create path \n",
        "    for img in os.listdir(path):  # iterate over each image \n",
        "        img_cap = image.imread(os.path.join(path,img))  # convert to array\n",
        "        img_cap = img_cap[300:580,450:730,:]\n",
        "        img_cap = rgb2hsv(img_cap)\n",
        "        #img_cap = img_cap[:, :, 0]    #hue\n",
        "        #img_cap = img_cap[:, :, 1]    #sat\n",
        "        img_cap = img_cap[:, :, 2]    #value\n",
        "        if category == 'pH_2':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_3':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_4':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_5':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_6':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_7':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_8':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_9':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        if category == 'pH_10':\n",
        "          x_db[ind,:,:] = img_cap\n",
        "        \n",
        "        ind = ind + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKfUnmyUiuwq"
      },
      "source": [
        "address3 = '/content/drive/MyDrive/Mestrado_Bruno Xavier/Dissertacao de Mestrado - Cod/Valor_pH-DB_med_pH_Alline_ajust.txt'\n",
        "arq = open(address3,'r')\n",
        "#y_db = np.zeros((tam,1),np.int16)\n",
        "y_db = np.zeros((tam,1),np.float)\n",
        "for i in range(tam):\n",
        "  linha = arq.readline()\n",
        "  val = linha.split()\n",
        "  y_db[i,:] = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXfp1_BD4ZhE"
      },
      "source": [
        "print('Tam vetor x: ',len(x_db))\n",
        "print('Tam vetor y: ',len(y_db))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdOqfXyCh45"
      },
      "source": [
        "# Pode ser preciso rodar duas vezez\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_db, y_db, test_size=0.3,random_state=1)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG0FWo4zF-Vk"
      },
      "source": [
        "print('Tam vetor y_train: ',len(y_train))\n",
        "#print(y_train)\n",
        "print('Tam vetor y_train: ',len(y_test))\n",
        "#print(y_test)\n",
        "print('Tam vetor y_val: ',len(y_val))\n",
        "#print(y_val)\n",
        "print('Tam vetor y: ',len(y_db))\n",
        "print('Tam vetor y_som: ',len(y_train)+len(y_test)+len(y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvNM_XCfItJM"
      },
      "source": [
        "Função para a criacao do modelo CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AJhDT6bqwdj"
      },
      "source": [
        "def Model(filt_siz_1,filt_siz_2,ker_siz_1,ker_siz_2,\n",
        "          n_layers,p_dropout,siz_dense_1,siz_dense_2,act_func_1,act_func_2):\n",
        "\n",
        "  #inputs = keras.layers.Input(shape=(800, 1280, 1))\n",
        "  inputs = keras.layers.Input(shape=(280, 280, 1))\n",
        "\n",
        "  if n_layers==2:\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation=act_func_1)(inputs)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation=act_func_2)(x)\n",
        "  if n_layers==3:\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation=act_func_1)(inputs)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation=act_func_1)(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation=act_func_2)(x)\n",
        "  if n_layers==4:\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation=act_func_1)(inputs)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation=act_func_1)(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation=act_func_2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation=act_func_2)(x)\n",
        "\n",
        "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "  x = keras.layers.Dense(siz_dense_1, activation='relu')(x)\n",
        "  x = keras.layers.Dropout(p_dropout)(x)\n",
        "  x = keras.layers.Dense(siz_dense_2, activation='relu')(x)\n",
        "  \n",
        "  outputs = keras.layers.Dense(1)(x)\n",
        "  #outputs = keras.layers.Dense(len(CATEGORIES), activation='softmax')(x)\n",
        "\n",
        "  return keras.models.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa7ItDFjI5oy"
      },
      "source": [
        "Treinamento do modelo com sweep wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbCFz8JnJqRB"
      },
      "source": [
        "#project_name=\"sweeps-CNN_regre_pH_teste_9-med-pH-Alline-ajust-crop-hsv_hue\"\n",
        "#project_name=\"sweeps-CNN_regre_pH_teste_9-med-pH-Alline-ajust-crop-hsv_sat\"\n",
        "project_name=\"sweeps-CNN_regre_pH_teste_9-med-pH-Alline-ajust-crop-hsv_value\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enMRWg9MI_aD"
      },
      "source": [
        "def train():\n",
        "    # Specify the hyperparameter to be tuned along with\n",
        "    # an initial value\n",
        "    config_defaults = {\n",
        "        'epochs': 60,\n",
        "        'batch_size': 8,\n",
        "        'n_layers': 2,\n",
        "        'activation_1': 'relu',\n",
        "        'activation_2': 'relu',\n",
        "        'filters_size_1':8,\n",
        "        'filters_size_2':8,\n",
        "        'kernel_size_1': 3,\n",
        "        'kernel_size_2': 3,\n",
        "        'p_dropout':0.2,\n",
        "        'learning_rate': 0.001,\n",
        "        'siz_dense_1':128,\n",
        "        'siz_dense_2':32\n",
        "    }\n",
        "\n",
        "    # Initialize wandb with a sample project name\n",
        "    wandb.init(config=config_defaults)\n",
        "\n",
        "    ## Prepare trainloader\n",
        "    trainloader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    trainloader = trainloader.shuffle(1024).batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # prepare valloader \n",
        "    valloader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    valloader = valloader.batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # prepare testloader \n",
        "    testloader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    testloader = testloader.batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Iniialize model with hyperparameters\n",
        "    keras.backend.clear_session()\n",
        "    model = Model(wandb.config.filters_size_1,wandb.config.filters_size_2, wandb.config.kernel_size_1,wandb.config.kernel_size_2,\n",
        "                  wandb.config.n_layers,wandb.config.p_dropout,wandb.config.siz_dense_1,wandb.config.siz_dense_2,\n",
        "                  wandb.config.activation_1,wandb.config.activation_2)\n",
        "    \n",
        "    model.summary()\n",
        "    # Compile the model\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=wandb.config.learning_rate) # optimizer with different learning rate specified by config\n",
        "    model.compile(opt, loss='mean_squared_error', metrics=[MeanAbsoluteError(),RootMeanSquaredError()])\n",
        "\n",
        "    # Train the model\n",
        "    treinamento = model.fit(trainloader, epochs=wandb.config.epochs, validation_data= valloader, callbacks=[WandbCallback()]) # WandbCallback to automatically track metrics\n",
        "    \n",
        "    #Performance\n",
        "    mse_train, mae_train, rmse_train = model.evaluate(trainloader)\n",
        "    mse_val, mae_val, rmse_val = model.evaluate(valloader)\n",
        "    mse_test, mae_test, rmse_test = model.evaluate(testloader)\n",
        " \n",
        "    wandb.log({\"MAE_train\": mae_train})\n",
        "    wandb.log({\"MSE_train\": mse_train})\n",
        "    wandb.log({\"RMSE_val\": rmse_train})\n",
        "    wandb.log({\"MAE_val\": mae_val})\n",
        "    wandb.log({\"MSE_val\": mse_val})\n",
        "    wandb.log({\"RMSE_val\": rmse_val})\n",
        "    wandb.log({\"MAE_test\": mae_test})\n",
        "    wandb.log({\"MSE_test\": mse_test})\n",
        "    wandb.log({\"RMSE_test\": rmse_test})\n",
        "    ##  TesteGeral\n",
        "    db_loader = tf.data.Dataset.from_tensor_slices((x_db, y_db))\n",
        "    db_loader = db_loader.shuffle(1024).batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    mse_db, mae_db, rmse_db = model.evaluate(db_loader)\n",
        "    wandb.log({\"MAE_geral\": mae_db})\n",
        "    wandb.log({\"MSE_geral\": mse_db})\n",
        "    wandb.log({\"RMSE_geral\": rmse_db})   \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5i0ulGXJUIV"
      },
      "source": [
        "Sweep config (Definição dos parametros a serem otimizados e suas faixas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmfP7QVxJccB"
      },
      "source": [
        "sweep_config = {\n",
        "  'method': 'bayes', \n",
        "  'metric': {\n",
        "      'name': 'val_loss',\n",
        "      'goal': 'minimize'\n",
        "  },\n",
        "  'early_terminate':{\n",
        "      'type': 'hyperband',\n",
        "      'min_iter': 50\n",
        "  },\n",
        "  'parameters': {\n",
        "        'epochs': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 120,\n",
        "            'min': 80\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [4,8,12]\n",
        "        },\n",
        "        'n_layers': {\n",
        "            'values': [2, 3, 4]\n",
        "        },\n",
        "        'activation_1': {\n",
        "            'distribution': 'categorical',\n",
        "            'values':['linear', 'relu', 'sigmoid','tanh']\n",
        "        },\n",
        "        'activation_2': {\n",
        "            'distribution': 'categorical',\n",
        "            'values':['linear', 'relu', 'sigmoid','tanh']\n",
        "        },\n",
        "        'filters_size_1': {\n",
        "            'values': [4, 8]\n",
        "        }, \n",
        "        'filters_size_2': {\n",
        "            'values': [4, 8]\n",
        "        },\n",
        "        'kernel_size_1': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        'kernel_size_2': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        'p_dropout': {\n",
        "            'values': [0.05,0.1,0.15,0.2,0.25,0.3]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
        "        },\n",
        "        'siz_dense_1': {\n",
        "            'values': [40,50,60,70,80,90,100,120]\n",
        "        },\n",
        "        'siz_dense_2': {\n",
        "            'values': [10,20,30,40,50,60]\n",
        "        }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FieXGSZEJi6b"
      },
      "source": [
        "Inicializar um novo projeto no sweep config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edlRPu63JqmJ"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=project_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1I6g_eGJsvV"
      },
      "source": [
        "Ativando o sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAoaXVfRxLDp"
      },
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
