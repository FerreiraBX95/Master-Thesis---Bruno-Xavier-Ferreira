{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ekwBHgAyFvBNrwlPc-q6DIffQ4Iv0NaA",
      "authorship_tag": "ABX9TyN6/ZtRc19SvpSBK2RfPJ0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FerreiraBX95/Master-Thesis---Bruno-Xavier-Ferreira/blob/Development-of-models-for-measurement-of-pH-in-atmospheric-and-pressurized-systems-using-artificial-intelligent-strategies/Code_for_training_the_classification_CNN_models_Input_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECaZA7FPMvOt"
      },
      "source": [
        "***  Code for training the classification CNN models - Input 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPzy34xmM4_O"
      },
      "source": [
        "Inicialização do Weights and Bias (WandB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pAqoNHKMuNW"
      },
      "source": [
        "# WandB – Install the W&B library\n",
        "%pip install wandb -q\n",
        "#!pip install wandb --upgrade\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVdrkQb4eXcQ"
      },
      "source": [
        "Informações da GPU a ser utilizada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp94ktVkd0P8"
      },
      "source": [
        "# Para usar a GPU eh preciso antes \"Alterar o tipo de ambiene de Execução\" para GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print('Informacao da GPU: ')\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5TMgnx6NJdT"
      },
      "source": [
        "Atualizações de programas (se necessario)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbeD6vlbMSg-"
      },
      "source": [
        "%pip install scikit-plot -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RsK5G-TvnHi"
      },
      "source": [
        "%pip install scikit-learn -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIGnXWulNKek"
      },
      "source": [
        "#!pip install keras --upgrade\n",
        "#!pip install tensorflow --upgrade\n",
        "#!pip install sklearn --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB7F4TUL19Vk"
      },
      "source": [
        "%pip install keras-metrics -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ftDi03NNTE"
      },
      "source": [
        "Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akyaH1BCNRlo"
      },
      "source": [
        "# Bibliotecas\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import   Conv2D\n",
        "from tensorflow.keras.layers import   MaxPooling2D\n",
        "from tensorflow.keras.layers import   GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import roc_auc_score \n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.metrics import fbeta_score \n",
        "from sklearn.metrics import precision_score \n",
        "from sklearn.metrics import recall_score \n",
        "\n",
        "from skimage.transform import resize\n",
        "from scikitplot.metrics import plot_roc\n",
        "from scikitplot.metrics import plot_confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLo0keISNZs1"
      },
      "source": [
        "Carregamento e pre-processamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "568rF2FENasx"
      },
      "source": [
        "## Teste para leitura de imagens do drive\n",
        "address = '/content/drive/MyDrive/Mestrado_Bruno Xavier/Dissertacao de Mestrado - Cod/Banco_de_Imagens_Curva_de_calib_PYTHON_med_pH_Alline_ajust'\n",
        "CATEGORIES = [\"pH_2\",\"pH_3\",\"pH_4\",\"pH_5\",\"pH_6\",\"pH_7\",\"pH_8\",\"pH_9\",\"pH_10\"]\n",
        "\n",
        "tam= 313\n",
        "x_db = np.zeros((tam,800,1280,3),np.int16)\n",
        "y_db = np.zeros((tam,1),np.int16)\n",
        "\n",
        "ind = 0\n",
        "for category in CATEGORIES:  \n",
        "    path = os.path.join(address,category)  # create path\n",
        "    for img in os.listdir(path):  # iterate over each image\n",
        "        img_cap = image.imread(os.path.join(path,img))  # convert to array\n",
        "        if category == 'pH_2':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 0\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_3':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 1\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_4':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 2\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_5':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 3\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_6':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 4\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_7':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 5\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_8':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 6\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_9':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 7\n",
        "          #ind = ind + 1\n",
        "        if category == 'pH_10':\n",
        "          x_db[ind,:,:,:] = img_cap\n",
        "          y_db[ind] = 8\n",
        "          #ind = ind + 1\n",
        "        ind = ind + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXfp1_BD4ZhE",
        "outputId": "4616b8fb-a99d-455e-a8ae-8a012d2860d7"
      },
      "source": [
        "print('Tam vetor x: ',len(x_db))\n",
        "print('Tam vetor y: ',len(y_db))\n",
        "#print(y_db)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tam vetor x:  313\n",
            "Tam vetor y:  313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdOqfXyCh45"
      },
      "source": [
        "# Pode ser preciso rodar duas vezez\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_db, y_db, test_size=0.3,random_state=1)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG0FWo4zF-Vk"
      },
      "source": [
        "print('Tam vetor y_train: ',len(y_train))\n",
        "print('Tam vetor y_train: ',len(y_test))\n",
        "print('Tam vetor y_val: ',len(y_val))\n",
        "print('Tam vetor y: ',len(y_db))\n",
        "print('Tam vetor y_som: ',len(y_train)+len(y_test)+len(y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmkwDrbHP4ab"
      },
      "source": [
        "Contagem de itens por classe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE8fRbm6P5TL"
      },
      "source": [
        "print(\"counts of label '0' (pH=2): {} \\n\".format(sum(y_db == 0)))\n",
        "print(\"counts of label '1' (pH=3): {}\".format(sum(y_db == 1))) \n",
        "print(\"counts of label '2' (pH=4): {}\".format(sum(y_db == 2))) \n",
        "print(\"counts of label '3' (pH=5): {}\".format(sum(y_db == 3))) \n",
        "print(\"counts of label '4' (pH=6): {}\".format(sum(y_db == 4))) \n",
        "print(\"counts of label '5' (pH=7): {}\".format(sum(y_db == 5))) \n",
        "print(\"counts of label '6' (pH=8): {}\".format(sum(y_db == 6))) \n",
        "print(\"counts of label '7' (pH=9): {}\".format(sum(y_db == 7))) \n",
        "print(\"counts of label '8' (pH=10): {}\".format(sum(y_db == 8)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvNM_XCfItJM"
      },
      "source": [
        "Função para a criacao do modelo CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4nXElZdIyc8"
      },
      "source": [
        "def Model(filt_siz_1,filt_siz_2,ker_siz_1,ker_siz_2,\n",
        "          n_layers,p_dropout,siz_dense_1,siz_dense_2):\n",
        "\n",
        "  inputs = keras.layers.Input(shape=(800, 1280, 3))\n",
        "\n",
        "  if n_layers==2:\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation='relu')(inputs)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation='relu')(x)\n",
        "  if n_layers==3:\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation='relu')(inputs)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation='relu')(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation='relu')(x)\n",
        "  if n_layers==4:\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation='relu')(inputs)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_1, kernel_size=(ker_siz_1,ker_siz_1), activation='relu')(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation='relu')(x)\n",
        "    x = keras.layers.Conv2D(filters=filt_siz_2, kernel_size=(ker_siz_2,ker_siz_2), activation='relu')(x)\n",
        "\n",
        "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "  x = keras.layers.Dense(siz_dense_1, activation='relu')(x)\n",
        "  x = keras.layers.Dropout(p_dropout)(x)\n",
        "  x = keras.layers.Dense(siz_dense_2, activation='relu')(x)\n",
        "  \n",
        "  outputs = keras.layers.Dense(len(CATEGORIES), activation='softmax')(x)\n",
        "\n",
        "  return keras.models.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa7ItDFjI5oy"
      },
      "source": [
        "Treinamento do modelo com sweep wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbCFz8JnJqRB"
      },
      "source": [
        "project_name=\"sweeps-CNN_pH_teste_37-med-pH-Alline-ajust\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enMRWg9MI_aD"
      },
      "source": [
        "def train():\n",
        "    # Specify the hyperparameter to be tuned along with\n",
        "    # an initial value\n",
        "    config_defaults = {\n",
        "        'epochs': 60,\n",
        "        'batch_size': 8,\n",
        "        'n_layers': 2,\n",
        "        'filters_size_1':8,\n",
        "        'filters_size_2':8,\n",
        "        'kernel_size_1': 3,\n",
        "        'kernel_size_2': 3,\n",
        "        'p_dropout':0.2,\n",
        "        'learning_rate': 0.001,\n",
        "        'siz_dense_1':128,\n",
        "        'siz_dense_2':32\n",
        "    }\n",
        "\n",
        "    # Initialize wandb with a sample project name\n",
        "    wandb.init(config=config_defaults)\n",
        "\n",
        "    ## Prepare trainloader\n",
        "    trainloader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    trainloader = trainloader.shuffle(1024).batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # prepare valloader \n",
        "    valloader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    valloader = valloader.batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    # prepare testloader \n",
        "    testloader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    testloader = testloader.batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Iniialize model with hyperparameters\n",
        "    keras.backend.clear_session()\n",
        "    model = Model(wandb.config.filters_size_1,wandb.config.filters_size_2, wandb.config.kernel_size_1,wandb.config.kernel_size_2,\n",
        "                  wandb.config.n_layers,wandb.config.p_dropout,wandb.config.siz_dense_1,wandb.config.siz_dense_2)\n",
        "    \n",
        "    model.summary()\n",
        "    # Compile the model\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=wandb.config.learning_rate) # optimizer with different learning rate specified by config\n",
        "    model.compile(opt, loss='sparse_categorical_crossentropy', metrics=['acc','accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    treinamento = model.fit(trainloader, epochs=wandb.config.epochs, validation_data= valloader, callbacks=[WandbCallback()]) # WandbCallback to automatically track metrics\n",
        "\n",
        "    # Evaluate    \n",
        "    #loss, accuracy = model.evaluate(testloader, callbacks=[WandbCallback()])\n",
        "    #print('Test Error Rate: ', round((1-accuracy)*100, 2))\n",
        "    #wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)}) # wandb.log to track custom metrics\n",
        "\n",
        "    predictions = model.predict(x_test)\n",
        "    wandb.log({\"ROC Curve\" : wandb.plot.roc_curve(y_test,predictions, labels=CATEGORIES)})\n",
        "\n",
        "    class_score_data = []\n",
        "    for test, pred in zip(y_test, predictions):\n",
        "      class_score_data.append([test, pred])\n",
        "    wandb.log({\"class_scores\": wandb.Table(data=class_score_data,columns=[\"test\", \"pred\"])})\n",
        "    \n",
        "    # Log Confusion Matrix\n",
        "    y_test_pred_class = np.argmax(predictions, axis=1)\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    #plot_confusion_matrix(y_test, y_test_pred_class,ax=ax) #scikitplot\n",
        "    plot_confusion_matrix(y_test, y_test_pred_class,ax=ax,normalize ='True') #scikitplot\n",
        "    wandb.log({\"confusion_matrix\": wandb.Image(fig)}, commit=False)\n",
        "\n",
        "    # Metricas\n",
        "    wandb.log({'pr': wandb.plots.precision_recall(y_test,predictions, labels=CATEGORIES)})\n",
        "    #wandb.log({\"f_beta_w\":wandb.plot.fbeta_score(y_test, y_test_pred_class,average='weighted',beta=0.5)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5i0ulGXJUIV"
      },
      "source": [
        "Sweep config (Definição dos parametros a serem otimizados e suas faixas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmfP7QVxJccB"
      },
      "source": [
        "sweep_config = {\n",
        "  'method': 'bayes', \n",
        "  'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "  },\n",
        "  'early_terminate':{\n",
        "      'type': 'hyperband',\n",
        "      'min_iter': 50\n",
        "  },\n",
        "  'parameters': {\n",
        "        'epochs': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 160,\n",
        "            'min': 80\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [4,8,12]\n",
        "        },\n",
        "        'n_layers': {\n",
        "            'values': [2, 3, 4]\n",
        "        },\n",
        "        'filters_size_1': {\n",
        "            'values': [4, 8]\n",
        "        }, \n",
        "        'filters_size_2': {\n",
        "            'values': [4, 8]\n",
        "        },\n",
        "        'kernel_size_1': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        'kernel_size_2': {\n",
        "            'values': [1,3,5]\n",
        "        },\n",
        "        'p_dropout': {\n",
        "            'values': [0.05,0.1,0.15,0.2,0.25,0.3]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
        "        },\n",
        "        'siz_dense_1': {\n",
        "            'values': [40,50,60,70,80,90,100,120]\n",
        "        },\n",
        "        'siz_dense_2': {\n",
        "            'values': [10,20,30,40,50,60]\n",
        "        }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FieXGSZEJi6b"
      },
      "source": [
        "Inicializar um novo projeto no sweep config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edlRPu63JqmJ"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=project_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1I6g_eGJsvV"
      },
      "source": [
        "Ativando o sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAoaXVfRxLDp"
      },
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
